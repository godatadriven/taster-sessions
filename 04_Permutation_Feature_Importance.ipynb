{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "450037f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<link rel=\"stylesheet\" href=\"https://marysia.nl/assets/GDD/css/custom.css\">\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional: change Jupyter Notebook theme to GDD theme\n",
    "from IPython.core.display import HTML\n",
    "HTML(url='https://gdd.li/jupyter-theme')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226bcc00",
   "metadata": {},
   "source": [
    "![footer_logo](https://marysia.nl/assets/GDD/css/logo.png)\n",
    "\n",
    "# Feature Importance Permutation\n",
    "\n",
    "In the previous notebook, we investigated the sensitivity of the model towards its features. In other words, what would happen if we kept all other features the same, but changed the value of one. In this notebook, we'll dive further into understanding what happens when a prediction is created, this time answering the question: _which variables contribute to this result the most?_\n",
    "\n",
    "We will consider the same model -- body mass prediction of a penguin, based on its bodily measurements and a few categorical features.\n",
    "\n",
    "\n",
    "### Outline\n",
    "1. [Built-in Random Forest Feature Importance](#ranfor)\n",
    "1. [Scikit-Learn Permutation Feature Importance](#scikit)\n",
    "1. [Permutation Feature Importance with Dalex](#dalex)\n",
    "1. [Exercise](#ex1)\n",
    "\n",
    "![](images/gentoo.jpg)\n",
    "\n",
    "\n",
    "\n",
    "<a id = 'ranfor'></a>\n",
    "\n",
    "# 1. Built-in Random Forest Feature Importance\n",
    "\n",
    "\n",
    "First, let's load in the data! We start out with a slightly simpler model than we've seen before -- one that only takes numeric features as its input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6058cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "penguins = (\n",
    "    pd.read_csv('data/penguins.csv')\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Set features & target.\n",
    "feature_columns = ['flipper_length_mm', 'bill_length_mm', 'bill_depth_mm']\n",
    "target = 'body_mass_g'\n",
    "\n",
    "# Set X and y\n",
    "X = penguins.loc[:, feature_columns]\n",
    "y = penguins.loc[:, target]\n",
    "\n",
    "# Split the dataset. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fac535",
   "metadata": {},
   "source": [
    "We want to investigate the built-in scikit-learn feature importance functionality. This means we need to choose a model that has `.feature_importances_` as an attribute after it has been fitted - a Random Forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654a6fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9041a59",
   "metadata": {},
   "source": [
    "Not bad. Slightly worse than the models we've seen before, but that is to be expected, as the model has less features to learn from.\n",
    "\n",
    "A Random Forest model in scikit-learn has an attribute `feature_importances_` after it has been fitted, that we can check out. The postfix `_` indicates that this attribute is only accessible/set after the model has trained. Why? Well, it would be impossible to view what features the model deems most important before it has seen any data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95ba4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245daf34",
   "metadata": {},
   "source": [
    "As mentioned previously, they are the average of the feature importances for all the trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46d0e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tree.feature_importances_ for tree in model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40a7c1f",
   "metadata": {},
   "source": [
    "Let's wrap these values in nice Pandas dataframe to plot them, so we can see what importance matches with what feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame(model.feature_importances_,   # the importances\n",
    "                            columns=['importance'],   # give the df something to sort by later\n",
    "                            index=X_train.columns.tolist()   # set the index to the feature names\n",
    "                            )\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ca2283",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.sort_values('importance').plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a6de8c",
   "metadata": {},
   "source": [
    "<a id = 'scikit'></a>\n",
    "\n",
    "# 2. Scikit-Learn Permutation Feature Importance\n",
    "\n",
    "This way of calculating feature importances is specific to models such as the Decision Tree and Random Forest, as it is based on decreasing impurity. It is not a model agnostic method.\n",
    "\n",
    "Scikit-learn, however, also provides a model-agnostic way of assessing feature importance called _permutation feature importance_. We will explain how this method works later, but for now let's just show the feature importances it computes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2db38e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(model, \n",
    "                               X_test,\n",
    "                               y_test,\n",
    "                               n_repeats=10)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee54b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_df = pd.DataFrame(result.importances_mean, \n",
    "                             columns=['importance'],\n",
    "                             index=X_test.columns.tolist())\n",
    "importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f7c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_df.sort_values('importance').plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f8b5a",
   "metadata": {},
   "source": [
    "We have now seen two techniques for computing feature importance, but we have only computed the importance of numerical features.\n",
    "\n",
    "What if we extend the model with some categorical features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ed1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set features & target.\n",
    "feature_columns = ['flipper_length_mm', 'bill_length_mm', 'bill_depth_mm', 'sex', 'island', 'species']\n",
    "target = 'body_mass_g'\n",
    "\n",
    "# Set X and y\n",
    "X = penguins.loc[:, feature_columns]\n",
    "y = penguins.loc[:, target]\n",
    "\n",
    "# Split the dataset. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('onehot', OneHotEncoder(), ['sex', 'species', 'island']),\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('ct', ct),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "pipeline_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84028a9",
   "metadata": {},
   "source": [
    "As expected, a better model! But let's see how we can grab those feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce52313",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pipeline_rf['model'].feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67741f83",
   "metadata": {},
   "source": [
    "Alright, there we have our feature importances.. but wait, there are a few more than we expect. Let's check how many features we had originally, and how many importances we have now. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of features: {len(X_train.columns)}')\n",
    "print(f'Number of feature importances: {len(feature_importances)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63de2558",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It seems like we have a few more importances than features. Why is that? \n",
    "\n",
    "Well, because we are using the one hot encoder to one-hot encode all our categorical features, we end up with a separate column for each possible category for each categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d2cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = pipeline_rf['ct'].get_feature_names_out().tolist()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame(feature_importances, \n",
    "                             columns=['feature importances'], \n",
    "                             index=feature_names)\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ed17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.sort_values('feature importances').plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36285b4e",
   "metadata": {},
   "source": [
    "We can also do this in a model-agnostic way with permuation feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5977e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(pipeline_rf, \n",
    "                               X_test,\n",
    "                               y_test,\n",
    "                               n_repeats=10)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame(result.importances_mean, \n",
    "                             columns=['importance'],\n",
    "                             index=X_test.columns.tolist())\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fba794",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.sort_values('importance').plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a96d98",
   "metadata": {},
   "source": [
    "Notice that with this technique we do not experience the previous issue: importances are only computed for the original features!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65884ba",
   "metadata": {},
   "source": [
    "<a id = 'dalex'></a>\n",
    "\n",
    "# 3. Permutation Feature Importance with Dalex\n",
    "\n",
    "Permutation Feature Importance is a model-agnostic feature importance calculation method built-in in scikit-learn itself. However, we can also calculate permutation feature importance with the dalex package.\n",
    "\n",
    "This requires us to create an Explainer object first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a422927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dalex \n",
    "\n",
    "explainer_rf = dalex.Explainer(pipeline_rf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34e462",
   "metadata": {
    "hidePrompt": true
   },
   "source": [
    "Now we can use the explainer with `model_parts` to do permutation feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4809da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    explainer_rf\n",
    "    .model_parts()\n",
    "    .plot()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad087f",
   "metadata": {},
   "source": [
    "\n",
    "<a id = 'ex1'></a>\n",
    "# 4. Exercise \n",
    "\n",
    "\n",
    "\n",
    "<div class=\"exercise\" markdown=\"1\">\n",
    "\n",
    "### Exercise 1\n",
    "#### Random Forest feature importances\n",
    "\n",
    "1. Consider the Random Forest `.feature_importances_` attribute. Can you think of a way to combine the various categories per feature (i.e. Sex=Male and Sex=Female) to one single importance measure? \n",
    "2. Examine how the impurity-based technique ranks the features in terms of importance. How does this compare to the scikit-learn permutation based technique? (hint: consider flipper length vs. sex) \n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"exercise\" markdown=\"1\">\n",
    "\n",
    "### Exercise 2\n",
    "#### Permutation Feature Importance\n",
    "\n",
    "Consider the scikit-learn permutation feature importance & dalex permutation feature importance.\n",
    "1. Is the order of which features are considered most important the same for both methods? \n",
    "1. Are the feature importance values in the same scale?\n",
    "\n",
    "Permutation Feature Importance calculates the importance of an individual feature. It does this by scoring the performance of the model on the original data based on some metric (R2, RMSE, etc.) and comparing this to an altered version of the dataset. The altered version of the dataset is created by shuffling all the values in one column. The intuition here is that if the overall performance of the model decreases due to the feature values being shuffled, the feature must be important. Likewise, if the overall performance of the model remains constant while the feature values are shuffled, that feature must not be very important. \n",
    "\n",
    "\n",
    "1. Why do you imagine the values differ per feature between the scikit-learn built-in permutation feature importance and the implementation in dalex? \n",
    "1. Can you think of a way to calculate the uncertainty of each feature importance? \n",
    "1. Can you think of any reason why this method may not reflect the true importance? \n",
    "1. Here we show the feature importances based on the _test_ data. Can you make an argument for and against calculating this on the test data (compared to the train data)?\n",
    "1. An alternative is _drop feature importance_. Can you imagine what this method does? Why would we prefer permutation feature importance? \n",
    "\n",
    "\n",
    "**Bonus**: Implement your own version of permutation feature importance. \n",
    "**Bonus**: Extend it with a measure of uncertainty! \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a695f4ef",
   "metadata": {},
   "source": [
    "Your answers here.\n",
    "\n",
    "Exercise 1: \n",
    "1. \n",
    "2. \n",
    "\n",
    "Exercise 2: \n",
    "1. \n",
    "2.\n",
    "---\n",
    "1.\n",
    "2.\n",
    "3. \n",
    "4. \n",
    "5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b653cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your (bonus) code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969c4a4",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
