{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chicken-orlando",
   "metadata": {},
   "source": [
    "<img src=images/gdd-logo.png align=right width=300px style='padding:20px'>\n",
    "\n",
    "\n",
    "# Hackathon: Laptop Prices\n",
    "Welcome to the hackathon! In this hackathon, you'll get the opportunity to try out your chosen explainability technique(s) on a dataset on laptop price prediction. \n",
    "\n",
    "\n",
    "### Outline\n",
    "1. [Problem Introduction](#intro)\n",
    "1. [About the data](#data)\n",
    "1. [Creating the model](#model) \n",
    "1. [Assignment](#assignment)\n",
    "\n",
    "<a id = 'intro'></a>\n",
    "\n",
    "## Problem Introduction\n",
    "\n",
    "You are about to be moved into a brand new team and everyone will need to buy a new laptop this time next year. Everyone has submitted some specifications they'd like their laptop to be (weight, RAM, memory, GPU, Manufacturer etc.) and you want to be able to estimate the cost of these new laptops.\n",
    "\n",
    "You have data on a collection laptops along with the prices that they are. Your model should be able to determine the price of the laptop based on the information you have.\n",
    "\n",
    "Since you want to keep costs down you want to be able to interpret your model so that you know which specifications/details cause the laptop's price to change the most and therefore what to suggest people compromise on the most to reduce costs.\n",
    "\n",
    "<img src=\"images/laptop.jpeg\" style=\"display: block;margin-left: auto;margin-right: auto;height: 200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-harvey",
   "metadata": {},
   "source": [
    "<a id = 'data'></a>\n",
    "\n",
    "## About the Data \n",
    "\n",
    "The features in the dataset are described below:\n",
    "\n",
    "|Column|Type|Description|\n",
    "|---|---|---|\n",
    "| company| String |Laptop Manufacturer|\n",
    "| product |String |Brand and Model|\n",
    "| type_name |String |Type (Notebook, Ultrabook, Gaming, etc.)|\n",
    "| inches |Numeric|Screen Size|\n",
    "| screen_resolution |String| Screen Resolution|\n",
    "| screen_resolution_width |String| Screen Resolution width only|\n",
    "| screen_resolution_height |String| Screen Resolution height only|\n",
    "| cpu| String |Central Processing Unit (CPU)|\n",
    "| ram |String|Laptop RAM in GB|\n",
    "| memory_disk |String|Hard Disk Memory in GB|\n",
    "| memory_ssd |String|SSD Memory|\n",
    "| gpu |String| Graphics Processing Units (GPU)|\n",
    "| op_sys |String| Operating System|\n",
    "| weight |String| Laptop Weight in kilograms|\n",
    "| price |Numeric| Price (Euro)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "laptops = pd.read_csv('data/laptops.csv', encoding = \"ISO-8859-1\")\n",
    "laptops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-transportation",
   "metadata": {},
   "source": [
    "<a id = 'model'></a>\n",
    "\n",
    "## Creating the model\n",
    "\n",
    "Split the data into `X` and `y` where `X` is the feature matrix and `y` is the target (`price`)\n",
    "\n",
    "Exclude `company`, `product` and `screen_resolution` from the feature matrix due to the large amount of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['laptop_id', 'type_name', 'inches',\n",
    "       'screen_resolution_width', 'screen_resolution_height', \n",
    "        'cpu', 'ram', 'memory_disk', 'memory_ssd', 'gpu', 'op_sys', 'weight']\n",
    "\n",
    "X = laptops.loc[:, features]\n",
    "y = laptops.loc[:, 'price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-gravity",
   "metadata": {},
   "source": [
    "Check the shape of `X` and `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-destruction",
   "metadata": {},
   "source": [
    "Perform the train test split on the data to create `X_train`, `X_test`, `y_train`, `y_test`\n",
    "\n",
    "Use a `random_state` to ensure the split is the same each time it is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=111)\n",
    "\n",
    "print('Shape of X_train and y_train', X_train.shape, y_train.shape)\n",
    "print('Shape of x_test and y_test', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-colleague",
   "metadata": {},
   "source": [
    "It seems as though we have some categorical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = X.select_dtypes('object').columns\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-uganda",
   "metadata": {},
   "source": [
    "Since there are categorical columns, we will need to encode these. First let's try out the one hot encoding. Since there are categorical columns, we will need to encode these. None are ordinal so we will use `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab5d0e9",
   "metadata": {},
   "source": [
    "Now check to see if there is any missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-plant",
   "metadata": {},
   "source": [
    "There are no missing values. \n",
    "\n",
    "Preprocessing needed:\n",
    "\n",
    "- Since we have categorical features that have no ranking, we will need to use `OneHotEncoder()`\n",
    "- Since we are building a `Linear Regression` we will want to `scale` the data so that the coefficients can be compared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-disaster",
   "metadata": {},
   "source": [
    "Now we need to build a column transformer so that we can only encode the categorical columns.\n",
    "\n",
    "- Import the `ColumnTransformer` from `sklearn.compose`\n",
    "- Instantiate the `ColumnTransformer()` with the `OneHotEncoder` on the categorical columns\n",
    "- Use the parameter `remainder='passthrough'` for the rest\n",
    "- use `.fit_transform()` with the column transformer on the `X_train` data and save this as `X_train_encoded`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('onehot', OneHotEncoder(drop='first', sparse=False), categorical_columns)\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train_encoded = column_transformer.fit_transform(X_train)\n",
    "X_train_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-organizer",
   "metadata": {},
   "source": [
    "Now let's try out a scaler. **You can only do this on the encoded data since you cannot scale categorical features!!**\n",
    "\n",
    "Choose from the below and import it in from `sklearn.preprocessing`\n",
    "\n",
    "- `StandardScaler`\n",
    "- `RobustScaler`\n",
    "- `MinMaxScaler`\n",
    "\n",
    "Instantiate your scaler (eg. `scaler = RobustScaler()`) and try it out by performing:\n",
    "\n",
    "```python\n",
    "pd.DataFrame(scaler.fit_transform(X_train_encoded), columns=column_transformer.get_feature_names())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pd.DataFrame(scaler.fit_transform(X_train_encoded), columns = column_transformer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-paper",
   "metadata": {},
   "source": [
    "Now that we have a scaler chosen, we're ready to build a pipeline.\n",
    "\n",
    "- Import `Pipeline` from `sklrean.pipeline` and `LinearRegression` from `sklearn.linear_model`.\n",
    "- Instantiate the model with no parameters\n",
    "- Instantiate the pipeline with the scaler and model as the 2 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "pipeline = Pipeline(steps = [\n",
    "    ('onehot', column_transformer),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', model)\n",
    "])\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-organ",
   "metadata": {},
   "source": [
    "Fit the pipeline to `X_train` and `y_train`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d8231",
   "metadata": {},
   "source": [
    "<a id = 'assignment'></a>\n",
    "\n",
    "# <mark>Assignment</mark>\n",
    "\n",
    "### Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de0f59",
   "metadata": {},
   "source": [
    "1. Read the problem description. Which type of explainability method do you imagine would be most suitable for this problem: \n",
    "    - Local (explains one single prediction) or global (explains model behaviour)? \n",
    "    - Feature importance (determining which features have the biggest impact on your predictions) or feature sensitivity (determining how predictions would be affected to changes in feature values)? \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea18b137",
   "metadata": {},
   "source": [
    "Write your answers here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b6596",
   "metadata": {},
   "source": [
    "2. Are there any inherently interpretable models that spring to mind that can help you address the need for explainability for this problem? The model implemented is Linear Regression. Was that a good choice?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7671cad7",
   "metadata": {},
   "source": [
    "Write your answers here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7fc98e",
   "metadata": {},
   "source": [
    "3. What model-agnostic techniques would be appropriate to address the need for explainability for this problem?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "135b993e",
   "metadata": {},
   "source": [
    "Write your answers here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e1ecc",
   "metadata": {},
   "source": [
    "#### Bonus \n",
    "4. Some explainability methods are less useful when features are highly correlated. Is that applicable to this dataset, and if so, what can you do to discover what features impact the laptop pricing the most?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b853204",
   "metadata": {},
   "source": [
    "Write your answers here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c32d0",
   "metadata": {},
   "source": [
    "### Do-it-yourself\n",
    "The explainability techniques covered in the workshop were: \n",
    "* Ceteris Paribus (local sensitivity)\n",
    "* Prediction Break-Down (local feature importance)\n",
    "* Permutation Feature Importance (global feature importance)\n",
    "* Partial Dependence Plots (global sensitivity)\n",
    "\n",
    "Implement the technique that you deem most appropriate for this problem. Consider both the problem statement, as well as the advantages and disadvantages of each method. Refer back to the [slides](https://github.com/marysia/explainability-workshop/blob/master/presentation.pdf) if necessary. \n",
    "\n",
    "#### Challenges:\n",
    "\n",
    "1. Create your own datapoint with a combination of laptop specifications. Use the pipeline to predict the price. \n",
    "\n",
    "2. Now imagine you want to cut the cost by \\$100. What change would you need to make to the laptop specifications to get that result?  \n",
    "\n",
    "3. Can you find out what laptop specifications, in general, contribute most to high price predictions? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Bonus challenges: \n",
    "* Also try out the other explainability techniques and see if you can discover something interesting.\n",
    "* Try out other models as well, and compare these \n",
    "* Extract the feature importance from the Linear Regression model using the coefficients. Does this match with the result of permutation feature importance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2206d09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "controlling-alfred",
   "metadata": {},
   "source": [
    "<img src='images/gdd-logo.png' align=right width=300px>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
