{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/gdd-logo.png align=right width=300px style='padding:20px'>\n",
    "\n",
    "# Hackathon: Discussion\n",
    "\n",
    "If you're feeling less like coding and more like discussing the theory of explainability then this hackathon is for you! Try to answer the questions below by discussing with your group.\n",
    "\n",
    "<img src=\"https://imgs.xkcd.com/comics/machine_learning_2x.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case\n",
    "1. In what scenarios do you imagine model explainability is important. Come up with one or more sector/use case. \n",
    "\n",
    "2. Which type of explainability method do you imagine would be most suitable for your use case: \n",
    "    - Local (explains one single prediction) or global (explains model behaviour)? \n",
    "    - Feature importance (determining which features have the biggest impact on your predictions) or feature sensitivity (determining how predictions would be affected to changes in feature values)? \n",
    "    \n",
    "3.  Are there any inherently interpretable models that spring to mind that can help you address the need for explainability for this problem?\n",
    "\n",
    "4. What model-agnostic techniques would be appropriate to address the need for explainability for this problem?\n",
    "\n",
    "\n",
    "## Theory \n",
    "* For global methods (permutation feature importance and partial dependence plots), an argument can be made to use these techniques on the *train* set or on the *test* set. What would be the difference between the two, and which is the better choice? Can you think of a use case/scenario where it is better to apply it on one or the other? \n",
    "\n",
    "* Some explainability techniques require access to the model itself (e.g. feature importances to be extracted from a Lienar Regression model or drop feature importance), while others only require access to the prediction API. Can you think of a scenario where this \n",
    "\n",
    "## Other Hackathon Datasets\n",
    "The other hackathons each have a **Theory** part. Read the problem descriptions and answer the questions. \n",
    "\n",
    "\n",
    "## Applicability\n",
    "* Are any of the interpretability techniques discussed today useful for your job or the data you work with? \n",
    "\n",
    "* All examples so far have been on tabular (rows and columns) data. How could you make models on images and text more explainable?\n",
    "\n",
    "* The package used today, dalex, has been around since 2020. Research what other packages are out there. What are their respective (dis)advantages compared to dalex? \n",
    "\n",
    "* The workshop today did not cover the entire range of explainability methods. Others include LIME and Shapley values. Choose one and research how it works, and how it can be applied. \n",
    "\n",
    "* What do you imagine makes a 'good explanation' of a machine learning model? How is that different from *algorithmic transparency*? \n",
    "\n",
    "* Early in the workshop, we talked about *example-based explanations*, one of which was *counterfactuals*. Counterfactuals tell us how a data point has to change to *significantly* change its prediction. What do you imagine the properties of a **good** counterfactual are? (hint: the change should be as small as possible, i.e. body weight decrease by 25g rather than decrease by 250g if both options flip the prediction). \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your answers & notes here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/gdd-logo.png' align=right width=300px>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
