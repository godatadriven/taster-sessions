{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "verbal-muslim",
   "metadata": {},
   "source": [
    "<img src=images/gdd-logo.png align=right width=300px style='padding:20px'>\n",
    "\n",
    "# Hackathon: Medical Diagnoses\n",
    "Welcome to the hackathon! In this hackathon, you'll get the opportunity to try out your chosen explainability technique(s) on a dataset on medical diagnoses. \n",
    "\n",
    "\n",
    "### Outline\n",
    "1. [Problem Introduction](#intro)\n",
    "1. [About the data](#data)\n",
    "1. [Creating the model](#model) \n",
    "1. [Assignment](#assignment)\n",
    "\n",
    "<a id = 'intro'></a>\n",
    "\n",
    "## Problem Introduction\n",
    "You are working for a hospital and you want to be able to detect a certain disease using data taken from medical images.\n",
    "\n",
    "You have data describing an image of a medical sample and knowledge of whether the patient the sample as taken from was diagnosed with the disease or not.\n",
    "\n",
    "You want to be able to capture every patient who need to be assessed further so as to rule out any complications related to the disease.\n",
    "\n",
    "\n",
    "<img src=\"images/doctor.png\" style=\"display: block;margin-left: auto;margin-right: auto;height: 300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-richardson",
   "metadata": {},
   "source": [
    "<a id = 'data'></a>\n",
    "\n",
    "## About the data\n",
    "\n",
    "This dataset comes from Scikit-Learn as one of the many datasets to explore and on which perform machine learning. [The Breast cancer wisconsin (diagnostic) dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-wisconsin-diagnostic-dataset).\n",
    "\n",
    "The features in the dataset were computed from a digitized image of a breast tissue sample; they describe characteristics of the cell nuclei present in the image.\n",
    "\n",
    "\n",
    "|Column|Description|Type|\n",
    "|:---|:---|:---|\n",
    "|id|ID number|float|\n",
    "|diagnosis|The diagnosis of breast tissues (M = malignant, B = benign)|float|\n",
    "|radius_mean|mean of distances from center to points on the perimeter|float|\n",
    "|texture_mean|standard deviation of gray-scale values|float|\n",
    "|perimeter_mean|mean size of the perimeter of the tumor|float|\n",
    "|area_mean|mean size of the tumor|float|\n",
    "|smoothness_mean|mean of local variation in radius lengths|float|\n",
    "|compactness_mean|mean of perimeter^2 / area - 1.0|float|\n",
    "|concavity_mean|mean of severity of concave portions of the contour|float|\n",
    "|concave points_mean|mean for number of concave portions of the contour|float|\n",
    "|fractal_dimension_mean|mean for \"coastline approximation\" - 1|float|\n",
    "|radius_se|standard error for the mean of distances from center to points on the perimeter|float|\n",
    "|texture_se|standard error for standard deviation of gray-scale values|float|\n",
    "|perimeter_se|standard error for the perimeter of the tumor|float|\n",
    "|area_se|standard error for the size of the tumor|float|\n",
    "|smoothness_se|standard error for local variation in radius lengths|float|\n",
    "|compactness_se|standard error for perimeter^2 / area - 1.0|float|\n",
    "|concavity_se|standard error for severity of concave portions of the contour|float|\n",
    "|concave points_se|standard error for number of concave portions of the contour|float|\n",
    "|fractal_dimension_se|standard error for \"coastline approximation\" - 1|float|\n",
    "|radius_worst|\"worst\" or largest mean value for mean of distances from center to points on the perimeter|float|\n",
    "|texture_worst|\"worst\" or largest mean value for standard deviation of gray-scale values|float|\n",
    "|perimeter_worst|\"worst\" or largest mean value for mean of perimeter|float|\n",
    "|area_worst|\"worst\" or largest mean value for mean of area|float|\n",
    "|smoothness_worst|\"worst\" or largest mean value for local variation in radius lengths|float|\n",
    "|compactness_worst|\"worst\" or largest mean value for perimeter^2 / area - 1.0|float|\n",
    "|concavity_worst|\"worst\" or largest mean value for severity of concave portions of the contour|float|\n",
    "|concave points_worst|\"worst\" or largest mean value for number of concave portions of the contour|float|\n",
    "|fractal_dimension_worst|\"worst\" or largest mean value for \"coastline approximation\" - 1|float|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "medical = pd.read_csv('data/medical.csv')\n",
    "medical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-queue",
   "metadata": {},
   "source": [
    "<a id = 'model'></a>\n",
    "## Creating the model\n",
    "#### Prepare `X` and `y`\n",
    "\n",
    "Split the data into `X` and `y` where `X` is the feature matrix and `y` is the target (`price`)\n",
    "\n",
    "Exclude `id` from the feature matrix due to it being a unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-brand",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "floating-credit",
   "metadata": {},
   "source": [
    "Check the shape of `X` and `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-cartoon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "streaming-fence",
   "metadata": {},
   "source": [
    "#### Train Test Split\n",
    "\n",
    "Perform the train test split on the data to create `X_train`, `X_test`, `y_train`, `y_test`\n",
    "\n",
    "Use a `random_state` to ensure the split is the same each time it is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-turkish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "representative-woman",
   "metadata": {},
   "source": [
    "Check the shape of `X_train`, `X_test`, `y_train` and `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-destination",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "minor-brighton",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "There are no categorical values or missing values to deal with. However since we are building a `Logistic Regression` we will want to `scale` the data so that the coefficients can be compared.\n",
    "\n",
    "Choose from the below and import it in from `sklearn.preprocessing`\n",
    "\n",
    "- `StandardScaler`\n",
    "- `RobustScaler`\n",
    "- `MinMaxScaler`\n",
    "\n",
    "Instantiate your scaler (eg. `scaler = RobustScaler()`) and try it out by performing:\n",
    "\n",
    "```python\n",
    "pd.DataFrame(scaler.fit_transform(X_train), columns=features)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-engine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "raising-tourist",
   "metadata": {},
   "source": [
    "#### Building the Model\n",
    "\n",
    "Now that we have a scaler chosen, we're ready to build a pipeline.\n",
    "\n",
    "- Import `Pipeline` from `sklearn.pipeline` and a model (e.g. `LogisticRegression` from `sklearn.linear_model`).\n",
    "- Instantiate the model with no parameters\n",
    "- Instantiate the pipeline with the scaler and model as the 2 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-thinking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "outside-casting",
   "metadata": {},
   "source": [
    "Fit the pipeline to `X_train` and `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-rolling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f3d8231",
   "metadata": {},
   "source": [
    "<a id = 'assignment'></a>\n",
    "\n",
    "# <mark>Assignment</mark>\n",
    "\n",
    "### Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de0f59",
   "metadata": {},
   "source": [
    "1. Model interpretability is often hugely important when machine learning models are being used for medical applications. Can you describe why you might want to use model explainability techniques for this particular dataset?  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "19f67a4c",
   "metadata": {},
   "source": [
    "Write your answers here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b2152c",
   "metadata": {},
   "source": [
    "2. Read the problem description. Which type of explainability method do you imagine would be most suitable for this problem: \n",
    "    - Local (explains one single prediction) or global (explains model behaviour)? \n",
    "    - Feature importance (determining which features have the biggest impact on your predictions) or feature sensitivity (determining how predictions would be affected to changes in feature values)? "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7240d31",
   "metadata": {},
   "source": [
    "Write your answers here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956bceb",
   "metadata": {},
   "source": [
    "3. Are there any inherently interpretable models that spring to mind that can help you address the need for explainability for this problem? The model suggested was Logistic Regression. Was that a good choice?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44048b51",
   "metadata": {},
   "source": [
    "Write your answers here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a3430",
   "metadata": {},
   "source": [
    "5. What model-agnostic techniques would be appropriate to address the need for explainability for this problem?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45c9793d",
   "metadata": {},
   "source": [
    "Write your answers here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5143a2",
   "metadata": {},
   "source": [
    "#### Bonus \n",
    "6. Some explainability methods are less useful when features are highly correlated. Is that applicable to this dataset, and if so, how? What do you imagine you could you do to mitigate this? "
   ]
  },
  {
   "cell_type": "raw",
   "id": "550fcc75",
   "metadata": {},
   "source": [
    "Write your answers here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf89cf5",
   "metadata": {},
   "source": [
    "### Do-it-yourself\n",
    "The explainability techniques covered in the workshop were: \n",
    "* Ceteris Paribus (local sensitivity)\n",
    "* Prediction Break-Down (local feature importance)\n",
    "* Permutation Feature Importance (global feature importance)\n",
    "* Partial Dependence Plots (global sensitivity)\n",
    "\n",
    "Implement the technique that you deem most appropriate for this problem. Consider both the problem statement, as well as the advantages and disadvantages of each method. Refer back to the [slides](https://github.com/marysia/explainability-workshop/blob/master/presentation.pdf) if necessary. \n",
    "\n",
    "\n",
    "\n",
    "#### Bonus challenges: \n",
    "* Also try out the other explainability techniques!\n",
    "* Use a Decision Tree model and visualise the decision tree. Experiment with various settings for max depth. How does this influence the model's performance and the degree of explainability? Is a Decision Tree a good choice for this problem, both in terms of performance and interpretability?\n",
    "* Try out any other model you'd like.\n",
    "* Example-based explanations:\n",
    "    * Can you try k-Nearest Neighbors classifier and extract the example data points that explain a specific prediction? \n",
    "    * Can you find a counterfactual? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb30ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "controlling-alfred",
   "metadata": {},
   "source": [
    "<img src='images/gdd-logo.png' align=right width=300px>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
