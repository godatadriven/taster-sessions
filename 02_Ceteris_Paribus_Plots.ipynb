{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69b3526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<link rel=\"stylesheet\" href=\"https://marysia.nl/assets/GDD/css/custom.css\">\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional: change Jupyter Notebook theme to GDD theme\n",
    "from IPython.core.display import HTML\n",
    "HTML(url='https://gdd.li/jupyter-theme')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1001c000",
   "metadata": {},
   "source": [
    "![footer_logo](https://marysia.nl/assets/GDD/css/logo.png)\n",
    "\n",
    "# Ceteris Paribus Plots\n",
    "\n",
    "In this notebook, we shall take our first steps towards *model-agnostic* interpretability methods. That is, methods of interpretability that are not reliant on the algorithm's innate interpretability. They can be used on any model, regardless of how easy the model is to understand by analysing the algorithm alone. \n",
    "\n",
    "We will address Ceteris Paribus plots, both on the Support Vector Classifier model we saw in the previous notebook, as well as on a new problem -- predicting the body mass of a given penguin, given its bodily measurements, sex and species. \n",
    "\n",
    "### Outline\n",
    "1. [Create the model](#model)\n",
    "1. [Ceteris Paribus](#cetpar)\n",
    "1. [Exercise 1](#ex1)\n",
    "1. [Regression & Categorical features](#regcat)\n",
    "1. [Exercise 2](#ex2)\n",
    "\n",
    "![](https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/logo.png)\n",
    "\n",
    "<a id = 'model'></a>\n",
    "\n",
    "# 1. Create the Model\n",
    "\n",
    "We start out by again loading in the data. This is the same as we saw in the previous notebook -- we load in the penguins dataset, drop the Adelie penguin for simplicity's sake, define the features (flipper length and body mass) and target (species). Based on that, we can create our feature matrix X and target vector y and split it into a train & test part. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c4ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load in the data. \n",
    "penguins = (\n",
    "    pd.read_csv('data/penguins.csv')\n",
    "    .loc[lambda d: d['species'] != 'Adelie']\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Define features & target\n",
    "feature_columns = ['flipper_length_mm', 'body_mass_g']\n",
    "target = 'species'\n",
    "\n",
    "# Set X and y\n",
    "X = penguins.loc[:, feature_columns]\n",
    "y = penguins.loc[:, target]\n",
    "\n",
    "# Split the dataset. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7246dd00",
   "metadata": {},
   "source": [
    "However, we need one small change compared to our previous workflow -- scikit-learn is able to handle both numeric input (integers) as well as strings (words) for the target variable. That meant that, in our previous notebook, we could keep the target variable as Chinstrap or Gentoo for each row. \n",
    "\n",
    "However, the explainability package that we will be using later on this notebook requires our target to be numeric -- so 0 and 1 instead of Chinstrap and Gentoo. Therefore, we must also train our model -- the model that is to be explained by our explainability package -- on numeric targets. In order to do this, we simply create a mapping where Chinstraps are to be represented with a zero and Gentoo penguins with a one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccecca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'Chinstrap': 0, 'Gentoo': 1}\n",
    "y_train = y_train.replace(mapping)\n",
    "y_test = y_test.replace(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e3e643",
   "metadata": {},
   "source": [
    "Other than changing the representation of our target variable, everything else related to training the model remains the same. We import the support vector classifier, instantiate it, fit it on our train set, and review the accuracy on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47791a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_svm = SVC()\n",
    "model_svm.fit(X_train, y_train)\n",
    "model_svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634e638a",
   "metadata": {},
   "source": [
    "Great! Even though we changed our y_train and y_test representation, the score remains the same. Now let's get some insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abac892",
   "metadata": {},
   "source": [
    "<a id = 'cetpar'></a>\n",
    "\n",
    "# 2. Ceteris Paribus Plots\n",
    "\n",
    "Let us grab our data point from the previous notebook, and see if we can find some kind of explanation of what would be needed to change this datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2825114",
   "metadata": {},
   "outputs": [],
   "source": [
    "flipper_length_mm = 195\n",
    "body_mass_g = 4400\n",
    "\n",
    "example_datapoint = pd.DataFrame.from_dict({\n",
    "    'flipper_length_mm': [flipper_length_mm], \n",
    "    'body_mass_g': [body_mass_g]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81292b22",
   "metadata": {},
   "source": [
    "![footer_logo](https://dalex.drwhy.ai/misc/dalex_even.png)\n",
    "\n",
    "Our SVM model in itself is not very interpretable. That is why we need an external tool.\n",
    "\n",
    "`dalex`, short for _moDel Agnostic Language for Exploration and eXplanation_, is a package for both Python and R that is part of the [DrWhy.AI](https://github.com/ModelOriented/DrWhy/blob/master/README.md) universe. DrWhy.AI concerns itself with the question \"How to develop machine learning models in a responsible manner?\" and one of the areas it focuses on is _transparency_. Does the user know what influences model predictions? If the model decisions affect us directly or indirectly, we should know where these decisions come from and how they can be changed. And that's where dalex comes in. \n",
    "\n",
    "First of all, we need to import the package and create an _Explainer_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd64df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dalex\n",
    "\n",
    "explainer = dalex.Explainer(model_svm, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caf64be",
   "metadata": {},
   "source": [
    "The Explainer object that we just created, creates a wrapper around a predictive model. Black-box models may have very different structures. Right now, we are using dalex with sklearn, but we may - for example - also use it with a Tensorflow model. \n",
    "\n",
    "The Explainer class creates a unified representation of a model, which can then be used or processed to produce various types of explainations that can be plotted. \n",
    "\n",
    "This means we pass it the model to be explained. Depending on the type of explanations that we want, we also pass it the data it was trained on -- X_train and y_train. This is not always necessary, but it is now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa1230",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = explainer.predict_profile(example_datapoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e3be86",
   "metadata": {},
   "source": [
    "Once we have our explainer, we can create a _profile_ given our single example datapoint. There are several types of profiles that we can create, but the default (and the one we will focus on for now) is _partial_. As you can see, it calculates the _ceteris paribus_ profile. \n",
    "\n",
    "Once the profile is created, we can easily plot it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298e7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd5563",
   "metadata": {
    "hideCode": true,
    "hideOutput": true
   },
   "source": [
    "<a id = 'ex1'></a>\n",
    "# 3. Exercise\n",
    "<div class=\"exercise\" markdown=\"1\">\n",
    "\n",
    "### Exercise \n",
    "#### Ceteris Paribus plots for classification\n",
    "\n",
    "Analyse the ceteris paribus plots of the two features created with dalex.\n",
    "\n",
    "1. What do you notice? Do these plots help you discover something interesting about the model?\n",
    "1. Flip the prediction: analyse the ceteris paribus plot and determine what values (flipper length and/or body mass) must be changed and how much in order to flip the prediction. **Note**: 0 is Chinstrap, 1 is Gentoo!\n",
    "1. Do you have an intuition on how the values are calculated? Do you think you would be able to reproduce one of the values, for instance, the prediction for body_mass_g = 4000? \n",
    "1. Describe in your own words what the ceteris paribus plots display.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the values to change the prediction. \n",
    "flipper_length_mm = 195\n",
    "body_mass_g = 4400\n",
    "\n",
    "example_datapoint = pd.DataFrame.from_dict({\n",
    "    'flipper_length_mm': [flipper_length_mm], \n",
    "    'body_mass_g': [body_mass_g]\n",
    "})\n",
    "\n",
    "svm_pred = model_svm.predict(example_datapoint)\n",
    "\n",
    "print(f'Prediction SVM: {svm_pred[0]}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd1ff04e",
   "metadata": {},
   "source": [
    "- What do you notice? Do these plots help you discover something interesting about the model?\n",
    "- Describe in your own words what the ceteris paribus plots diplay.\n",
    "\n",
    "Write your answers here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b61404",
   "metadata": {},
   "source": [
    "\n",
    "<a id = 'regcat'></a>\n",
    "\n",
    "# 4. Regression & Categorical Features\n",
    "![footer_logo](https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/lter_penguins.png) \n",
    "\n",
    "\n",
    "Our initial problem was relatively simple: predict the penguin species (Gentoo or Chinstrap) based on two features. Let's investigate a slightly more complex problem instead, and see how Ceteris Paribus plots can help us understand what's going on. \n",
    "\n",
    "We are going to redefine our feature matrix X and target vector y to create a _regression_ problem. That is, a problem for which the outcome is numeric, rather than a category. Our target will be to predict _a given penguin's body mass in grams_. \n",
    "\n",
    "In addition, we will use a combination of _numeric_ and _categorical_ features, and include all three penguin species -- including the Adélie penguin! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24416b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data again - do *not* drop Adélie! \n",
    "penguins = (\n",
    "    pd.read_csv('data/penguins.csv')\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Set features & target.\n",
    "feature_columns = ['flipper_length_mm', 'bill_length_mm', 'bill_depth_mm', 'sex', 'island', 'species']\n",
    "target = 'body_mass_g'\n",
    "\n",
    "# Set X and y\n",
    "X = penguins.loc[:, feature_columns]\n",
    "y = penguins.loc[:, target]\n",
    "\n",
    "# Split the dataset. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e04d7",
   "metadata": {},
   "source": [
    "Creating our model becomes a little bit more complicated, because we need a way to process the newly introduced _categorical_ features (the penguin's sex, the island it was sighted on, and the species) to a numeric format. This is because scikit-learn expects all its input to be numeric. The way we choose to process this is with _one-hot encoding_.\n",
    "\n",
    "One-hot encoding -- also known as dummy encoding -- entails adding binary variables for every unique category. In the case of the *island* column, this means 3 extra columns will be created because there are 3 possible islands. \n",
    "\n",
    "In this case, we use ColumnTransformer to encode our data, because we can then indicate which columns we want to one-hot encode (only the categorical ones). \n",
    "\n",
    "Next, we create a pipeline containing two steps: the ColumnTransformer we just created and the model we want to use -- in this case a random forst regressor. The beauty of pipelines is that they can be treated like any other model, which means we can plug it in anywhere we would an estimator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b111a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('onehot', OneHotEncoder(), ['sex', 'species', 'island']),\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('ct', ct),\n",
    "    ('model', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a9515",
   "metadata": {},
   "source": [
    "Now that we have our trained model, in the form of a sklearn pipeline, we can again create an Explainer object for this model. Why do we have to create a new object and can we not use the previously created `explainer` variable? Because each Explainer that is created is specific to the model/algorithm that was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d897954",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = dalex.Explainer(pipeline, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a16f3f0",
   "metadata": {},
   "source": [
    "Let us now at random choose a row from our test set to understand. Feel free to adjust it to pick another row! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 230\n",
    "row = X_test.loc[[230]]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87164a75",
   "metadata": {},
   "source": [
    "We now again calculate a ceteris paribus profile for the chosen row, and immediately plot the profile to show how flipper length, bill length and bill depth influence the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f58dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = explainer.predict_profile(row)\n",
    "profile.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867045c",
   "metadata": {},
   "source": [
    "However, these are just the _numeric_ features. What about the categorical ones? \n",
    "\n",
    "We can investigate the categorical features by adding an argument to the plotting method. Notice that we do not have to re-calculate the profile! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2afca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.plot(variable_type=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a63fbc",
   "metadata": {},
   "source": [
    "<a id = 'ex2'></a>\n",
    "# 5. Exercise\n",
    "<div class=\"exercise\" markdown=\"1\">\n",
    "\n",
    "### Exercise \n",
    "#### Ceteris Paribus plots\n",
    "\n",
    "Analyse the ceteris paribus plots of the numeric and categorical features created with dalex.\n",
    "\n",
    "1. Look at the categorical features. What features and associated values increase the model's prediction of body weight? Which ones decrease the model's prediction? Which features hardly have any impact? \n",
    "2. Can you reproduce one of the graphs? E.g. the predictions for the three possible species. \n",
    "3. Execute the cells below with the linear regression model. Compare the Ceteris Paribus plots of the linear regression model to the random forest regressor. Can you explain why they look different? \n",
    "4. Compare the Ceteris Paribus plots for the Island variable for the Random Forest regressor and the Linear Regression model. Are they the same or different? Can you explain why?\n",
    "5. Can you think of any circumstances in which a Ceteris Paribus plot would be helpful? Can you think of any downsides? Write down advantages and disadvantages.\n",
    "\n",
    "**Bonus**: Go back a few cells and choose a different value for _n_ (the row to choose). Rerun the plots. Do they look the same or different? _Hint:_ use `X_test.sample()` to take a random data point in the test set, and set _n_ to the value in the index.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bd2c170",
   "metadata": {},
   "source": [
    "Your answers here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8412eeb",
   "metadata": {},
   "source": [
    "Additional code for the exercise: \n",
    "\n",
    "In this case, we are creating a new model -- this time not a Random Forest regressor, but a linear regression model. Notice how we rename the pipeline variable to `pipeline_lr` to denote it is our linear regression model pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab28ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('onehot', OneHotEncoder(), ['sex', 'species', 'island']),\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipeline_lr = Pipeline([\n",
    "    ('ct', ct),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "pipeline_lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f5aec",
   "metadata": {},
   "source": [
    "We must create a new explainer as well, one for our linear regression pipeline. Notice how we do not overwrite the original explainer, but create one specifically for our linear regression model called `explainer_lr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b18c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_lr = dalex.Explainer(pipeline_lr, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2004ecb",
   "metadata": {},
   "source": [
    "Create and plot a profile for our linear regression pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa3e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_lr = explainer_lr.predict_profile(row)\n",
    "profile_lr.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431cbf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_lr.plot(variable_type=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc6061",
   "metadata": {},
   "source": [
    "------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
