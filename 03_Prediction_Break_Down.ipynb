{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0cd157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<link rel=\"stylesheet\" href=\"https://marysia.nl/assets/GDD/css/custom.css\">\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional: change Jupyter Notebook theme to GDD theme\n",
    "from IPython.core.display import HTML\n",
    "HTML(url='https://gdd.li/jupyter-theme')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37138126",
   "metadata": {},
   "source": [
    "![footer_logo](https://marysia.nl/assets/GDD/css/logo.png)\n",
    "\n",
    "# Break-Down Plots\n",
    "\n",
    "In the previous notebook, we investigated the sensitivity of the model towards its features. In other words, what would happen if we kept all other features the same, but changed the value of one. In this notebook, we'll dive further into understanding what happens when a prediction is created, this time answering the question: _which variables contribute to this result the most?_\n",
    "\n",
    "We will consider the same model -- body mass prediction of a penguin, based on its bodily measurements and a few categorical features.\n",
    "\n",
    "### Outline\n",
    "1. [Create the model](#model)\n",
    "1. [Prediction Break-Down Plots](#breakdown)\n",
    "1. [Exercise](#ex1)\n",
    "\n",
    "![](https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/logo.png)\n",
    "\n",
    "<a id = 'model'></a>\n",
    "\n",
    "# 1. Create the Model\n",
    "\n",
    "We start out by again loading in the data. This is the same as we saw in the previous notebook -- we load in the penguins dataset, set our feature matrix X and target y, and divide up our data in a train and test split. Notice that by setting the random state, we ensure we have the same train/test split in every notebook! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read in the data again - do *not* drop Ad√©lie! \n",
    "penguins = (\n",
    "    pd.read_csv('data/penguins.csv')\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Set features & target.\n",
    "feature_columns = ['flipper_length_mm', 'bill_length_mm', 'bill_depth_mm', 'sex', 'island', 'species']\n",
    "target = 'body_mass_g'\n",
    "\n",
    "# Set X and y\n",
    "X = penguins.loc[:, feature_columns]\n",
    "y = penguins.loc[:, target]\n",
    "\n",
    "# Split the dataset. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77aa6d8",
   "metadata": {},
   "source": [
    "We once again create our random forest regressor model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22667df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('onehot', OneHotEncoder(), ['sex', 'species', 'island']),\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('ct', ct),\n",
    "    ('model', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "pipeline_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5192d1",
   "metadata": {},
   "source": [
    "<a id = 'breakdown'></a>\n",
    "# 3. Prediction Break-Down Plots \n",
    "\n",
    "Let us select a row at random. Again, feel free to choose a different data point from the test set! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f57f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 230\n",
    "row = X_test.loc[[230]]\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e5bd5d",
   "metadata": {},
   "source": [
    "Our explainer is once again created. This time, we are a little bit more explicit in our variable naming -- both our pipeline and explainer have `_rf` as a postfix to indicate it refers to our Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733cfa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dalex \n",
    "\n",
    "explainer_rf = dalex.Explainer(pipeline_rf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45107774",
   "metadata": {},
   "source": [
    "This is where things change -- whereas previously we used `.predict_profile` to calculate the Ceteris Paribus plot, in this case, we use `.predict_parts`. This takes a few arguments:\n",
    "* `new_observation`: this is an observation that was not in the original train set, which we would like to understand.\n",
    "* `type`: options are 'break_down_interactions', 'break_down', 'shap' and 'shap_wrapper'. The default is 'break_down_interactions', though we will set it at 'break_down'\n",
    "* `label`: this is simply what will appear at the top of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c51e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_explanation = explainer_rf.predict_parts(\n",
    "    row,\n",
    "    type=\"break_down\",\n",
    "    label=f\"row {n}\",\n",
    ")\n",
    "\n",
    "\n",
    "row_explanation.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905dd1d1",
   "metadata": {},
   "source": [
    "<a id = 'ex1'></a>\n",
    "# 3. Exercise \n",
    "\n",
    "<div class=\"exercise\" markdown=\"1\">\n",
    "\n",
    "### Exercise \n",
    "#### Analyse Prediction Break Down\n",
    "\n",
    "Try to answer the following questions: \n",
    "\n",
    "1. **Plot analysis**: What does the color green in the plot indicate? What does the color red in the plot indicate? What does the color blue in the plot indicate?\n",
    "1. **Order dependence**: Copy the cell that creates the row explanation. In the copied cell, add `order=X_train.columns.tolist()` as an argument. What changes about the plot? Why? Are the values the same for each feature, or have the values changed? \n",
    "1. **Linear regression**: Create a breakdown plot for a linear regression model for the same n. How is it different from the random forest explanation? \n",
    "1. **Order dependence for linear regression**: Copy the cell that creates the row explanation. In the copied cell, add `order=X_train.columns.tolist()` as an argument. Does it change in the way you expect? Are the values the same for each feature, or have the values changed? \n",
    "1. **Advantages & Disadvantages**: How does the break-down plot compare to the ceteris paribus plot seen before? What questions can you answer with this plot, that you could not answer with the ceteris paribus plot? What can you explain with the Ceteris Paribus plot that you cannot explain with the break-down plot? In summary, what are the respective advantages and disadvantages.\n",
    "\n",
    "**Bonus**: Why do you think order dependence matters for a random forest, but not for linear regression?\n",
    "\n",
    "**Bonus**:  Can you explain how the breakdown is calculated? Can you reproduce the first three numbers of the explanation? (4192.021, +741.452, +210.558 for n = 230, but you may choose a different value for n)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c8becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Linear Regression pipeline\n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('onehot', OneHotEncoder(), ['sex', 'species', 'island']),\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipeline_lr = Pipeline([\n",
    "    ('ct', ct),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "pipeline_lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12550f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your break-down plot for linear regression here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82779e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your break-down plot for linear regression with adjusted order variable here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e9008",
   "metadata": {},
   "source": [
    "------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
